### MIPT LLM

| Дата     |    Семинар                                  | Презентации            |     Домашнее задание |
| ---------|-------------------------------------------------------| -----------------------|----------------------|
| `7.11.24`  |**Введение в NLP, word2vec**<p>На этом семинаре мы подробно обсудили понятие контекста и контекстного эмбеддинга, разобрали подход word2vec, основанный на построении скипграмм и предсказании вероятности встретить одно слово в контексте с другим | [Слайды с семинара](https://github.com/YanaPropad/MIPT_LLM/blob/main/Введение%20в%20NLP%2C%20LLM%20%20в%20материаловедении%20и%20химии%20-%20часть%201.pdf)|&#128193;[homework_1](https://github.com/YanaPropad/MIPT_LLM/tree/main/homework_1)|
| `14.11.24` |**RNN, Attention, LLM в материаловедении и биохимии**<p>На этом семинаре мы перешли от слова к последовательности слов и рассмотрели задачу генерации последовательностей с помощью RNN (рекуррентных нейронных сетей), а также механизм Attention | [Слайды с семинара](https://github.com/YanaPropad/MIPT_LLM/blob/main/Введение%20в%20NLP%2C%20LLM%20в%20материаловедении%20и%20химии%20-%20часть%202.pdf)|&#128193;[homework_2](https://github.com/YanaPropad/MIPT_LLM/tree/main/homework_2)|

### Материалы для углубления в NLP и LLM:
1. Word2vec
   - Оригинальная статья по Word2vec с ICLR 2013:<br>
     [Mikolov, Tomas & Corrado, G.s & Chen, Kai & Dean, Jeffrey. (2013). Efficient Estimation of Word Representations in Vector Space. 1-12.](https://arxiv.org/abs/1301.3781)
   - Word2vec в картинках, оригинальные материалы, автор Jay Allamar:<br>
     [The Illustrated Word2vec](https://jalammar.github.io/illustrated-word2vec/)<br>
     и перевод на Хабре:<br>
     [Word2vec в картинках](https://habr.com/ru/articles/446530/)
2. Рекуррентные нейронные сети: RNN, LSTM, GRU и т.д.:
   - Быстрый разбор и сравнение архитектур рекуррентных нейронных сетей на русском:<br>
     [RNN, LSTM, GRU и другие рекуррентные нейронные сети](http://vbystricky.ru/2021/05/rnn_lstm_gru_etc.html)
3. Attention:
   - Оригинальная статья по механизму Attention с NeurIPS 2017:
     [Vaswani, Ashish, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. (2017) Attention is All you Need.](https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
4. Трансформеры:
   - Transformer в картинках, оригинальные материалы, автор Jay Allamar:<br>
     [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)<br>
     и перевод на Хабре:<br>
     [Transformer в картинках](https://habr.com/ru/articles/486358/)
     Материалы содержат в себе также подробное описание механизмов Attention, Self-Attention, Multihead-Attention
5. LLM:
  [Build a Large Language Model (From Scratch)](https://github.com/rasbt/LLMs-from-scratch):<br>
  Официальный репозиторий для книги "Build a Large Language Model (From Scratch)", Sebastian Raschka. Содержит задания, решения и объяснения каждого шага полного цикла разработки, претрейнинга и файнтюнинга GPT-подобных моделей
  ![Иллюстрация к проекту](https://camo.githubusercontent.com/a17472f25db0af2e7a72700cf3e994b48a61405931b54111ed4d62cbe0371216/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f6d656e74616c2d6d6f64656c2e6a7067)

