{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практика: word2vec для кристаллов\n",
    "\n",
    "Как вы уже могли заметить, идея, лежащая в основе [word2vec](https://arxiv.org/pdf/1310.4546), достаточно общая. В данном задании вы реализуете его самостоятельно.\n",
    "\n",
    "P.s. GPU в этом задании нам не потребуется :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многие функции из этого ноутбука лежат в файле `skipgram_model_functions.py`. Это сделано специально, чтобы подсветить важные моменты и не отвлекаться на большое количество кода. Мы рекомендуем ничего не менять в функциях, исключением могут быть функции для визуализации данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u7c/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import umap\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "from skipgram_model_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что мы хотим сделать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это задание нужно для того, чтобы своими руками написать модель, которая является отправной точкой для современных LLM. Прежде чем углубляться в более сложные архитектуры и изучать то, как LLM на лету генерирует текст (а иногда и не текст, а что-то другой модальности), что можно считать работой с последовательностями динамической длины, мы хотим разобраться с построением контекстуального эмбеддинга для токена - атомарной единицы языка. Мы будем решать задачу предсказания вероятности одного токена оказаться (или не оказаться) в одном контексте с другим. Из этого знания мы можем получить представление о том, какие токены схожи и различны. Опираясь на эту информацию можно не только приступать к генерации последовательностей, но и решать другие задачи. Например, в кристаллохимии можно предсказывать физикохимические свойства соединений, зная их векторное представление, или эмбеддинг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Небольшой рассказ о данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать с базой данных `Materials Project`. Для задачи были отобраны стабильные экспериментальные кристаллические структуры, для которых было сгенерировано текстовое описние при помощи `Robocrystallographer`. Все текстовые описания были токенизированы (в нашем случае токен = 1 слово). В `корпус` попали те структуры, описание которых не превышало 70 токенов. \n",
    "<p>Папка содержит 2 файла с данными:\n",
    "\n",
    "`robocrys_exp_stable_70_tokens.txt` - основной файл, с которым мы будем работать. Содержит строки с текстовым описанием кристаллических структур.\n",
    "<p>\n",
    "\n",
    "`robocrys_exp_stable_70_tokens.csv` - файл, содержащий дополнительную информацию о кристаллических структурах, которая пригодится при визуализации эмбеддингов, которые мы посчитаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(open(\"robocrys_exp_stable_70_tokens.txt\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных. Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация – первый шаг.\n",
    "Тексты, с которыми мы работаем, включают в себя пунктуацию и прочие нестандартные токены, так что простой `str.split` иногда может не подходить под конкретную задачу. В данном случае в качестве токена мы будем использовать каждое слово, число и символ пространнственной группы. Можно было бы для каждой строки `i` в `data` выполнить `data[i].strip().split()`, но мы используем `WhitespaceTokenizer`, выполняющий аналогичную функцию.\n",
    "\n",
    "Обратимся к `nltk` - библиотеке, которая нашла широкое применение в области NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выглядит типичное описание одного кристалла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_description = data[0]\n",
    "\n",
    "print(test_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что описание содержит переносы строки, запятые и точки, которые необходимо удалить перед применением `WhitespaceTokenizer`. Все точки удалять нельзя, потому что некоторые из них являются частью чисел с плавающей точкой (например, 3.26 Å). Также нельзя удалять и использовать в качестве разделителей для токенизации некоторые специальные символы: /, -."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала заменим все запятые в описаниях кристаллов на пробелы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_description = test_description.replace(\",\", \"\")\n",
    "\n",
    "print(test_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь необходимо избавиться от символов переноса строки и точек в конце каждого предложения. Для этого используем `PunktSentenceTokenizer`. Он разобьёт описания на предложения, а затем мы удалим точку в конце каждого из них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer = PunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = sentence_tokenizer.tokenize(test_description)\n",
    "\n",
    "print(f'Test sentences in description look like:\\n{test_sentences}')\n",
    "\n",
    "\n",
    "# Вот теперь используем WhiteSpaceTokenizer\n",
    "sentences_modified = [s.rstrip('.') for s in test_sentences]\n",
    "test_tokens = sum([tokenizer.tokenize(sentence) for sentence in sentences_modified], [])\n",
    "\n",
    "print(f'Test tokens of description look like:\\n{test_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь токенизируем все описания кристаллов в нашем корпусе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokenized = []\n",
    "\n",
    "for description in data:\n",
    "    description = description.replace(\",\", \"\")\n",
    "    sentences = sentence_tokenizer.tokenize(description)\n",
    "    sentences_modified = [s.rstrip('.') for s in sentences]\n",
    "    tokens = sum([tokenizer.tokenize(s) for s in sentences_modified], [])\n",
    "    data_tokenized.append(tokens)\n",
    "\n",
    "\n",
    "print(f\"Количество описаний кристаллов в корпусе:\\n{len(data_tokenized)}\\n\")\n",
    "\n",
    "n_tokens = sum(len(description) for description in data_tokenized)\n",
    "print(f\"Суммарное количество токенов в корпусе:\\n{n_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Частота встречаемости слов и уникальные слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для расчета вероятностей нахождения одного слова в контексте другого нам необходимо получить частоту встречаемости каждого токена и список уникальных токенов. Сначала подсчитаем частоту встречаемости слов в корпусе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_dict = get_word_count_dict(data_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим множество уникальных токенов. Это множество будет нашим `словарём`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(word_count_dict.keys())\n",
    "\n",
    "\n",
    "print(f'Количество уникальных токенов в корпусе:\\n{len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Индексируем наш словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word: index for index, word in enumerate(vocabulary)}\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание пар `(слово, контекст)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для построения пар `(слово, контекст)` нам необходимо пройтись скользящим окном по всем описаниям в нашем корпусе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже задана ширина окна контекста Skipgram модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_radius = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пары `(слово, контекст)` на основе доступного датасета сгенерированы ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_pairs = get_context_pairs(data_tokenized, word_to_index, window_radius)\n",
    "\n",
    "print(f\"Количество пар (слово, контекст): {len(context_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подзадача №1: subsampling\n",
    "Для того, чтобы сгладить разницу в частоте встречаемсости слов, необходимо реализовать механизм subsampling'а.\n",
    "Для этого вам необходимо реализовать функцию ниже.\n",
    "\n",
    "Вероятность **исключить** слово из обучения (на фиксированном шаге) вычисляется как\n",
    "$$\n",
    "P_\\text{drop}(w_i)=1 - \\sqrt{\\frac{t}{f(w_i)}},\n",
    "$$\n",
    "где $f(w_i)$ – нормированная частота встречаемости слова, а $t$ – заданный порог (threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_frequent_words(word_count_dict, threshold=1e-5):\n",
    "    \"\"\"\n",
    "    Calculates the subsampling probabilities for words based on their frequencies.\n",
    "\n",
    "    This function is used to determine the probability of keeping a word in the dataset\n",
    "    when subsampling frequent words. The method used is inspired by the subsampling approach\n",
    "    in Word2Vec, where each word's frequency affects its probability of being kept.\n",
    "\n",
    "    Parameters:\n",
    "    - word_count_dict (dict): A dictionary where keys are words and values are the counts of those words.\n",
    "    - threshold (float, optional): A threshold parameter used to adjust the frequency of word subsampling.\n",
    "                                   Defaults to 1e-5.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are words and values are the probabilities of keeping each word.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    words_count = sum(word_count_dict.values())\n",
    "\n",
    "    keep_prob_dict = {}  \n",
    "\n",
    "    for word, count in word_count_dict.items():\n",
    "\n",
    "        freq_norm = count / words_count\n",
    "        include_prob = (threshold / freq_norm) ** 0.5 if freq_norm > threshold else 0\n",
    "        keep_prob_dict[word] = include_prob\n",
    "        \n",
    "    return keep_prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подзадача №2: negative sampling\n",
    "Для более эффективного обучения необходимо не только предсказывать высокие вероятности для слов из контекста, но и предсказывать низкие для слов, не встреченных в контексте. Для этого вам необходимо вычислить вероятность использовать слово в качестве negative sample, реализовав функцию ниже.\n",
    "\n",
    "`ВНИМАНИЕ: этот код вам нужно написать самостоятельно`\n",
    "\n",
    "В оригинальной статье предлагается оценивать вероятность слов выступать в качестве negative sample согласно распределению $P_n(w)$\n",
    "$$\n",
    "P_n(w) = \\frac{U(w)^{3/4}}{Z},\n",
    "$$\n",
    "\n",
    "где $U(w)$ распределение слов по частоте (или, как его еще называют, по униграммам), а $Z$ – нормировочная константа, чтобы общая мера была равна $1$.\n",
    "\n",
    "`ПОДСКАЗКА`: распределение слов по частоте есть ничто иное как частота встречаемости слова в корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_sampling_prob(word_count_dict):\n",
    "    \"\"\"\n",
    "    Calculates the negative sampling probabilities for words based on their frequencies.\n",
    "\n",
    "    This function adjusts the frequency of each word raised to the power of 0.75, which is\n",
    "    commonly used in algorithms like Word2Vec to moderate the influence of very frequent words.\n",
    "    It then normalizes these adjusted frequencies to ensure they sum to 1, forming a probability\n",
    "    distribution used for negative sampling.\n",
    "\n",
    "    Parameters:\n",
    "    - word_count_dict (dict): A dictionary where keys are words and values are the counts of those words.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are words and values are the probabilities of selecting each word\n",
    "            for negative sampling.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return negative_sampling_prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим функции `subsample_frequent_words` для расчета вероятностей слов быть отобранными в обучающую выборку и `get_negative_sampling_prob` для расчета вероятностей слов быть отобранными в выборку negative_sampling.<p>\n",
    "Для удобства, преобразуем полученные словари в массивы (т.к. все слова все равно уже пронумерованы)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob_dict = subsample_frequent_words(word_count_dict)\n",
    "assert keep_prob_dict.keys() == word_count_dict.keys()\n",
    "\n",
    "keep_prob_array = keep_prob_to_array(keep_prob_dict, index_to_word, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sampling_prob_dict = get_negative_sampling_prob(word_count_dict)\n",
    "assert np.allclose(sum(negative_sampling_prob_dict.values()), 1)\n",
    "\n",
    "negative_sampling_prob_array = negative_sampling_prob_to_array(negative_sampling_prob_dict, index_to_word, word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация модели Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, время реализовать модель!\n",
    "\n",
    "Напомним, что в случае negative sampling решается задача максимизации следующего функционала:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\log \\sigma({\\mathbf{v}'_{w_O}}^\\top \\mathbf{v}_{w_I}) + \\sum_{i=1}^{k} \\mathbb{E}_{w_i \\sim P_n(w)} \\left[ \\log \\sigma({-\\mathbf{v}'_{w_i}}^\\top \\mathbf{v}_{w_I}) \\right],\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $\\mathbf{v}_{w_I}$ – вектор центрального слова $w_I$,\n",
    "- $\\mathbf{v}'_{w_O}$ – вектор слова из контекста $w_O$,\n",
    "- $k$ – число negative samples,\n",
    "- $P_n(w)$ – распределение negative samples, заданное выше,\n",
    "- $\\sigma$ – сигмоида."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModelWithNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, center_words, pos_context_words, neg_context_words):\n",
    "\n",
    "        center_embeddings = self.center_embeddings(center_words)\n",
    "        pos_context_embeddings = self.context_embeddings(pos_context_words)\n",
    "        neg_context_embeddings = self.context_embeddings(neg_context_words)\n",
    "\n",
    "        pos_scores = torch.sum(center_embeddings * pos_context_embeddings, dim=1)\n",
    "        neg_scores = torch.sum((center_embeddings.unsqueeze(1) * neg_context_embeddings), dim = 2)\n",
    "\n",
    "        return pos_scores, neg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучаем модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем модель. Не забываем инициализировать её перед каждым обучением!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "embedding_dim = 32\n",
    "num_negatives = 15\n",
    "\n",
    "model = SkipGramModelWithNegSampling(vocab_size, embedding_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=150)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "params_counter = 0\n",
    "for weights in model.parameters():\n",
    "    params_counter += weights.shape.numel()\n",
    "assert params_counter == len(word_to_index) * embedding_dim * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем обучение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 2500\n",
    "batch_size = 1024\n",
    "\n",
    "train_skipgram_with_neg_sampling(\n",
    "    model,\n",
    "    context_pairs,\n",
    "    keep_prob_array,\n",
    "    word_to_index,\n",
    "    batch_size,\n",
    "    num_negatives,\n",
    "    negative_sampling_prob_array,\n",
    "    steps,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем состояние обученной модельки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'my_model_100_test.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестируем модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим Accuracy модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_model(model, context_pairs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, взглянем на ближайшие по косинусной мере слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_word = \"He\"\n",
    "n_neighbours = 20\n",
    "\n",
    "find_nearest(model, central_word, word_to_index, index_to_word, n_neighbours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала нам необходимо из всего списка слов отобрать те, которые мы хотим визуализировать. \n",
    "<p>Мы хотим визуализировать формулы кристаллов, отберем их: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_formules = [description[0] for description in data_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем эмбеддинги для отобранных слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = get_word_embeddings(model, word_to_index, list_of_formules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наши эмбеддинги имеют размерность 32, а мы хотим визуализировать их на интерактивном 3D-графике. Для этого необходимо понизить размерность эмбеддингов с помощью редьюсеров. Мы будем использовать `umap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(n_components=3, n_neighbors=5)\n",
    "\n",
    "embeddings_3d = umap_model.fit_transform(word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим интерактивные 3D-графики.\n",
    "Ниже вы можете ввести название файла, в который будет сохранен интерактивный график. Открыть его можно на локальном компьютере в браузере. Все остальные переменные лучше не трогать, но если хочется настроить график под себя, можно исправить функции в файле `skipgram_model_functions.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"3D_plot_by_structure_types_70_tokens.html\"\n",
    "\n",
    "properties_filename = 'robocrys_exp_stable_70_tokens.csv'\n",
    "get_3D_plot_by_structure_types(list_of_formules, embeddings_3d, accuracy, output_filename, properties_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"3D_plot_by_crystal_system_70_tokens.html\"\n",
    "\n",
    "properties_filename = 'robocrys_exp_stable_70_tokens.csv'\n",
    "get_3D_plot_by_crystal_system(list_of_formules, embeddings_3d, accuracy, output_filename, properties_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Над чем стоит подумать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Как повысить accuracy?\n",
    "<li> Возможно, вы заметили в токенизации некоторые баги. Подумайте, с помощью каких инструментов от них удобнее всего избавиться.\n",
    "<li> Как ширина контекстного окна Skipgram модели влияет на результат? Что изменится, если его уменьшить/увеличить?\n",
    "<li> Если получилось отметить явно выраженные кластеры на 3D-графиках, подумать, какая закономерность в том, что эмбединги структур, находящихся в кластере, схожи."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystallm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
